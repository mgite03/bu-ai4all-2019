{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of  Lesson 5_ Naive Bayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgite03/bu-ai4all-2019/blob/main/Copy_of_Lesson_5__Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl57sDm6om2n"
      },
      "source": [
        "# Practice Problems\n",
        "## Lesson 5: Naive Bayes\n",
        "---\n",
        "Created by Terron Ishihara"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz4bc5YuotbW"
      },
      "source": [
        "### Problem 1 - Probability\n",
        "\n",
        "Suppose we have two bags containing four types of gems: diamond, garnet, amethyst, and pearl (which may ring a bell for any [Steven Universe](https://en.wikipedia.org/wiki/Steven_Universe) fans). We'll call these Bag A and Bag B. The event of randomly selecting a diamond from either bag is denoted with a capital D. The same goes for G for garnet, A for amethyst, and P for Pearl.\n",
        "\n",
        "> Each bag contains the following number of each gem:\n",
        "\n",
        "| Bag A | Bag B |\n",
        "|------------|------------------|\n",
        "| 2 diamonds | 0 diamonds |\n",
        "| 4 garnets | 6 garnets |\n",
        "| 1 amethyst | 3 amethysts |\n",
        "| 0 pearls | 2 pearls |\n",
        "\n",
        "> **For each of the following questions, be sure to answer using the proper notation for probabilities.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvR5FXX_tc9p"
      },
      "source": [
        "> a. What is the sample space of these bags?\n",
        "\n",
        "The sample space of A is 7 and of B is 11. <<<<< nope!\n",
        "\n",
        "\n",
        "> A= {diamonds, garnets, amethyst}\n",
        ">B= {garnets, amethysts, pearls}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFG7wb4nuhFA"
      },
      "source": [
        "> b. What is the probability of drawing a diamond from Bag A? What is the probability of drawing an amethyst from Bag B? What is the probability of drawing an amethyst or a garnet from Bag A?\n",
        "\n",
        ">P(D|A) = 2/ 7\n",
        "\n",
        ">P(A|B) = 3/11\n",
        "\n",
        ">P(A or G|A) = 5/7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsBS1w04vlMR"
      },
      "source": [
        "> c. What is the probability of drawing a garnet from Bag A and drawing a pearl from Bag B? Are these events independent?\n",
        "\n",
        "These two events are mutually exclusive. P(G from A) = 4/7 and (P from B = 2/11) so 4/7 * 2/11 = 8 / 77"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZukuYDdwPsk"
      },
      "source": [
        "> d. What is the probability of drawing two diamonds from Bag B? That is, drawing a diamond, then drawing a second diamond without replacing the first? Are these events independent?\n",
        "\n",
        "uh there are no diamonds in bag b, therefore the probability of drawing two diamonds from bag b is 0 %. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MvdsFH9x0NH"
      },
      "source": [
        "> e. What are the union and intersection of Bag A and Bag B? Provide an example of an event which produces an empty set of outcomes.\n",
        "\n",
        "> A union B = {diamonds,  garnets,  amethysts, and  pearls.} \n",
        "\n",
        "> A intersection B  =  { garnets,   amethyst}\n",
        "\n",
        "> A intersection B - ???????????????????????????????????????????????????????????????"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_bxg4xD2TZT"
      },
      "source": [
        "### Problem 2 - Naive Bayes\n",
        "\n",
        "Let's take a look at how a Naive Bayes classifier words by working through the classifier's computation process step by step. To do this, we're going to make use of a library called NLTK (Natural Language Toolkit). This is a very useful tool for doing natural language processing, the subfield of machine learning that deals with understanding language.\n",
        "\n",
        "The example task we're going to work with is known as sentiment analysis. We will use a dataset of 2000 movie reviews (1000 positive, 1000 negative) provided by NLTK. In this case, the classification is binary: a review is either positive or negative (i.e. the reviewer liked or didn't like the movie). Let's start by importing the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMYH62jn2_Pq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2364a40c-5344-4a19-87a7-a92cee6c51af"
      },
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "from nltk.corpus import movie_reviews as mr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lA3YPsmfBC1I"
      },
      "source": [
        "> Now we can go through each positive and negative movie review and count how many times each word appears in the dataset. We need these counts to estimate the probabilities of a given word appearing in either a positive or negative review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytJC_MFt_k2t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "57c51903-c87e-4360-cf95-25cbdbc14864"
      },
      "source": [
        "# defaultdict is a handy type of dictionary where each key has a default value.\n",
        "# In this case, the keys in the dictionaries will be strings (words), and\n",
        "# by default their value will be a default int value of 0 (word count)\n",
        "from collections import defaultdict\n",
        "\n",
        "pos_word_counts = defaultdict(int)\n",
        "neg_word_counts = defaultdict(int)\n",
        "\n",
        "# Iterate over the file names in the dataset\n",
        "for file_name in mr.fileids():\n",
        "  # The part before the '/' in the file name is the label, either \"pos\" or \"neg\"\n",
        "  label = file_name.split('/')[0]\n",
        "  # Iterate over the words in the file\n",
        "  review_words = mr.words(file_name)\n",
        "  for word in review_words:\n",
        "    # Increment the appropriate word count dictionary\n",
        "    if label == \"pos\":\n",
        "      pos_word_counts[word] += 1\n",
        "    if label == \"neg\":\n",
        "      neg_word_counts[word] += 1\n",
        "      \n",
        "# Print the first 10 word counts in positive reviews\n",
        "for word, count in list(pos_word_counts.items())[:10]:\n",
        "  print(word, count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "films 884\n",
            "adapted 28\n",
            "from 2731\n",
            "comic 221\n",
            "books 49\n",
            "have 2240\n",
            "had 721\n",
            "plenty 76\n",
            "of 18636\n",
            "success 126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c7k1oLgCxwS"
      },
      "source": [
        "> In order to calculate probabilities, we need to know how many words there are total for each set, positive and negative reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qihhaFj7BZH6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4e0a26c1-9992-4a9f-a5cc-87aec1ee3e93"
      },
      "source": [
        "pos_word_total = 0\n",
        "neg_word_total = 0\n",
        "\n",
        "for count in pos_word_counts.values():\n",
        "  pos_word_total += count\n",
        "for count in neg_word_counts.values():\n",
        "  neg_word_total += count\n",
        "\n",
        "print(\"Total number positive words: \", pos_word_total)\n",
        "print(\"Total number negative words: \", neg_word_total)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number positive words:  832564\n",
            "Total number negative words:  751256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHgkIsRADrpN"
      },
      "source": [
        "> Consider a new movie review that we want to classify as positive or negative: *“This is the best movie I have ever seen.”* Naturally, we expect our classifier to say that this is a positive review. Recall that we make an independence assumption, that every word is independent from all other words. Given this, let's work out the math.\n",
        "\n",
        "> We ultimately want to calculate P(positive | review). That is, given this review, what is the probability that it is a positive review. To do this, we use Bayes' rule, which states:\n",
        "\n",
        "> P(positive | review) = P(review | positive) * P(positive) / P(review)\n",
        "\n",
        "> We want to do the same for P(negative | review), which follows the same formula. Since both quantities contain P(review) as the denominator, we can essentially ignore it in our calculations. P(positive) and P(negative) are simply the number of words in positive and negative reviews, respectively, which we already calculated. P(review | positive) and P(review | negative) are given by the independence assumption, allowing us to multiply the probabilities that each word in the new review would appear in a positive or negative review.\n",
        "\n",
        "> Let's see what this looks like in code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFGvMRznDhX2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "3e7b769e-43dd-4320-8cac-7af413475e40"
      },
      "source": [
        "def classify(review):\n",
        "  # Split the string on ' ' to get a list of words\n",
        "  review_words = review.split(' ')\n",
        "  \n",
        "  p_review_given_pos = 1 # P(review | pos)\n",
        "  for word in review_words:\n",
        "    # Calculate P(word_n | pos)\n",
        "    p_word_given_pos = pos_word_counts[word.lower()] / pos_word_total\n",
        "    # Multiply with overall product\n",
        "    p_review_given_pos *= p_word_given_pos\n",
        "  p_pos = p_review_given_pos * pos_word_total # P(pos | review)\n",
        "    \n",
        "  # Do the same for negative reviews\n",
        "  p_review_given_neg = 1\n",
        "  for word in review_words:\n",
        "    p_word_given_neg = neg_word_counts[word.lower()] / pos_word_total\n",
        "    p_review_given_neg *= p_word_given_neg\n",
        "  p_neg = p_review_given_neg * neg_word_total # P(neg | review)\n",
        "  \n",
        "  print(p_pos)\n",
        "  print(p_neg)\n",
        "    \n",
        "  # Compare the two probabilities and return the appropriate classification\n",
        "  if p_pos > p_neg:\n",
        "    return 'positive'\n",
        "  elif p_neg > p_pos:\n",
        "    return 'negative'\n",
        "  else:\n",
        "    return \"Not sure\"\n",
        "\n",
        "  \n",
        "print(classify('This movie was the greatest!!'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "0.0\n",
            "Not sure\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a33_MVUcKBwE"
      },
      "source": [
        "> a. Replace the review in the code above with something that you would expect to return negative as a classification. Does our model agree?\n",
        "\n",
        ">\"This is the worst movie I've ever seen\" - negative\n",
        "\n",
        ">\"This is terrible.\" - negative\n",
        "\n",
        ">\"This movie was the greatest!\" - negative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQyK2pF-KdMP"
      },
      "source": [
        "> b. Now try using review that contains a word that does not appear in the dataset. For example, \"Thanos is a great villain\" (\"Thanos\" does not appear in any reviews in this dataset). By default, the classification will be `negative`, but can you explain why? *(Hint: print out the values of `p_pos` and `p_neg`.)* \n",
        "\n",
        "> Both values were 0.0, so p_pos wasn't greater than p_neg. therefore, it went to the else statement, returning negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__YPMe7EMGWa"
      },
      "source": [
        "> c. (Challenge question) How can we go about resolving the predicament in part B? That is, if the review we want to classify contains a word that does not appear in our dataset, what can we do to make our model still calculate a reasonable prediction? Make these augmentations to the `classify()` method above. \n",
        "\n",
        "> Maybe just have and if, elif, and a \"not sure\" category for else. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmqWJwyY-d2a"
      },
      "source": [
        "> d. (Challenge question) One step that we have skipped when processing the words in each file is removing tokens (which basically means \"words\", but includes non-words too) like punctuation marks and what are known as \"stop words\". Stop words are words like \"the\", \"at\", \"is\", etc. These are words that are extremely common, so the probability that we calculate for these words is unnecessarily high. For example, \"The movie is great\" should place the most emphasis on the word \"great\", not the words \"The\" and \"is\".\n",
        "\n",
        "> The code below shows how you can import a collection stop words for the English language and a collection of punctuation marks. Use these to update the code above so that stop words and punctuation are ignored when counting words. \n",
        "\n",
        "> *Note: for the resulting code to work, your answer for part C must be implemented in the code as well, since stop words will no longer appear in our dataset.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ1FnLcA_P-8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e01c5273-b02a-499a-b56a-b02d5fc06ace"
      },
      "source": [
        "# Import the stopwords corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "# Get the stopwords for English\n",
        "stops = stopwords.words('english')\n",
        "\n",
        "# With string imported, you can access string.punctuation\n",
        "# which is a collection of punctuation marks.\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4eDD7A1J8B0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "03148b9e-3a0b-4942-f8da-cc5a6c20b7ec"
      },
      "source": [
        "def classify(review):\n",
        "  # Split the string on ' ' to get a list of words\n",
        "  review_words = review.split(' ')\n",
        "  print(review_words)\n",
        "  print(stops)\n",
        "  \n",
        "  for j in review_words:\n",
        "      if j in stops:\n",
        "        review_words.remove(j)\n",
        "        \n",
        "        \n",
        "  print(review_words)\n",
        "  \n",
        "  \n",
        "  p_review_given_pos = 1 # P(review | pos)\n",
        "  for word in review_words:\n",
        "    # Calculate P(word_n | pos)\n",
        "    p_word_given_pos = pos_word_counts[word.lower()] / pos_word_total\n",
        "    # Multiply with overall product\n",
        "    p_review_given_pos *= p_word_given_pos\n",
        "  p_pos = p_review_given_pos * pos_word_total # P(pos | review)\n",
        "    \n",
        "  # Do the same for negative reviews\n",
        "  p_review_given_neg = 1\n",
        "  for word in review_words:\n",
        "    p_word_given_neg = neg_word_counts[word.lower()] / pos_word_total\n",
        "    p_review_given_neg *= p_word_given_neg\n",
        "  p_neg = p_review_given_neg * neg_word_total # P(neg | review)\n",
        "  \n",
        "  print(p_pos)\n",
        "  print(p_neg)\n",
        "    \n",
        "  # Compare the two probabilities and return the appropriate classification\n",
        "  if p_pos > p_neg:\n",
        "    return 'positive'\n",
        "  elif p_neg > p_pos:\n",
        "    return 'negative'\n",
        "  else:\n",
        "    return \"Not sure\"\n",
        "\n",
        "  \n",
        "print(classify('this movie was adapted so well from the book. it was the best'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this', 'movie', 'was', 'adapted', 'so', 'well', 'from', 'the', 'book.', 'it', 'was', 'the', 'best']\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['movie', 'adapted', 'well', 'book.', 'was', 'the', 'best']\n",
            "0.0\n",
            "0.0\n",
            "Not sure\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zGj6stbRxpb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
