{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Intro to RL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgite03/bu-ai4all-2019/blob/main/rl/Copy_of_Intro_to_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diNS3CUJ25s7"
      },
      "source": [
        "# Intro to Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpcE5Cu9VpJ0"
      },
      "source": [
        "## What is Reinforcement Learning?\n",
        "\n",
        "Reinforcement learing is a type of machine learning that's different from both supervised and unsupervised learning. In reinforcement learning, we want to teach an agent to suceed in a certain environment by providing rewards to the agent.\n",
        "\n",
        "For example: say we want to teach a puppy to come when called. The agent is the puppy, and the environment is the world. When we call the puppy's name, it can take some action, either running over or ignoring its owner's voice. If the puppy comes running over, it receives a treat as its reward.\n",
        "\n",
        "<!-- Here's another example:  -->\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cvDFBY74fUM"
      },
      "source": [
        "## Breaking it down further...\n",
        "\n",
        "Every RL problem can be broken down into the following components:\n",
        "* Agent\n",
        "* Actions (that the agent can take)\n",
        "* Environment\n",
        "* State (of the environment)\n",
        "* Reward\n",
        "\n",
        "**Agent**: The agent is what we are trying to teach. It encompases everything we *can* control about the situation\n",
        "\n",
        "**Environment**: The environment is the world in which the agent resides. It encompases everything we *can't* control, although the agent can change the state of its environment by taking actions.\n",
        "\n",
        "**Actions**: Anything the agent can *do*. Whenever the agent takes an action, it can change the state of the environment.\n",
        "\n",
        "**State**: A complete description of the current state of the environment. By \"complete description\", we really mean everything, which includes not only where all of the mountains and trees are, but also that the agent is in a certain location currently climbing up a mountain with a certain speed, for example.\n",
        "\n",
        "**Reward**: A scalar (number) that the agent receives after each action. We give positive reward to \"good\" actions and negative reward to \"bad\" actions.\n",
        "\n",
        "(Read each of these descriptions over a few times to make sure you understand them.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLJIKNt2S39o"
      },
      "source": [
        "### Markov Decision Process\n",
        "\n",
        "A markov decision process (MDP) is a formulation of states and actions where the probability of moving to the next state from the current state depends only on the chosen action and is independent of any previous states.\n",
        "\n",
        "The states and actions of our RL problems follow this rule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyxGrS2qNUL2"
      },
      "source": [
        "## Moving through time\n",
        "\n",
        "![alt text](https://lh3.google.com/u/0/d/1ENx_BQnHLyeDF245SWuOzhk04C5xlFeZ=w2560-h1478-iv2)\n",
        "\n",
        "The agent observes the state of its environment, the agent takes an action, the state of the environment changes and the agent receives a reward, the agent takes another action based on the new state, and so on.\n",
        "\n",
        "Each one of these cycles happens in a **timestep**.\n",
        "\n",
        "Sequences of states and actions are broken down into **episodes**, and each episode lasts a certain number of timesteps. An episode can end when the agent dies or achieves its ultimate goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIlsOLYLgn-Z"
      },
      "source": [
        "# Representing all this in code\n",
        "\n",
        "We will start by importing the `gym` module created by OpenAI. `gym` contains many pre-coded environments that we can interact with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehQuuiDc5PzV"
      },
      "source": [
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GXY1Gkkg9td"
      },
      "source": [
        "We will be using the \"Frozen Lake\" environment. Read about what this environment is [here](https://gym.openai.com/envs/FrozenLake-v0/).\n",
        "\n",
        "Let's create an instance of the \"Frozen Lake\" environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv58nRZee9JO"
      },
      "source": [
        "env = gym.make(\"FrozenLake-v0\", is_slippery=False, map_name = \"8x8\", desc = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvtUvBdbiHCq"
      },
      "source": [
        "We can visualize the environment by calling the `render()` function on the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDz2UsbVh5vS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "cbbfc5bb-ebaf-4c3f-e828-8622cd263c02"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw3ZyUcliX6C"
      },
      "source": [
        "The highlighted square shows the current location of the agent. In this environment, the agent always starts on the \"start\" square (denoted by `S`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAOR9AW-k8EA"
      },
      "source": [
        "If we assign a number each square of the environment, we can describe the agent's state with just one number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgyfFgQGlr7x"
      },
      "source": [
        "Let's look at all the possible states:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SVgDfU9lEOc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70c078a4-b3b8-40d1-fc03-087622fb3e34"
      },
      "source": [
        "env.observation_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQgLeQH-lJ1I"
      },
      "source": [
        "`Discrete(16)` means that the agent can be in one of 16 possible states. A 4 by 4 grid has 16 squares."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtqYfHGAhMnP"
      },
      "source": [
        "Environments in `gym` have a few functions we can call:\n",
        "* `step(a)` tells the environment that the agent has chosen to take some action `a`. The environment calculates the next state depending on `a`, and it returns a tuple containing the new state, the reward, whether the episode has finished, and a dictionary of any additional information (which we won't be using).\n",
        "* `reset()` resets the environment to a fresh state, and returns the current state (which the agent can observe). For the Frozen Lake environment, this always puts the agent back at the upper-left corner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-_utuNJmMAV"
      },
      "source": [
        "Let's reset the environment and get the current state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRVEIt-Ge_zd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9994886c-ffbb-4f69-b975-cecd6d6cb559"
      },
      "source": [
        "obs = env.reset()\n",
        "print(\"current state =\", obs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current state = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4pWPZQimTvP"
      },
      "source": [
        "This is telling us that right now the agent is in state `0`, which is the upper left corner. We can verify this by rendering the environment again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03t3U_ffmlfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "10359c85-f012-4231-ed44-3459bb85c26f"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPbYkCCefCYz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a286773e-425d-4438-fa2c-07f74cb463a3"
      },
      "source": [
        "env.action_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G41E0mhgmp2O"
      },
      "source": [
        "This means that there are 4 possible actions that the agent can take. These happen to be left, down, right, and up. When we use `step()` to tell the environment which action we want to take, we input `0` for left, `1` for down, `2` for right, and `3` for up.\n",
        "\n",
        "For example, if we want to tell the agent to go down, we call `env.step(`1`)`.\n",
        "\n",
        "Tell the environment we want the agent to go down, and render the environment again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uOHIWNcfHD5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "fd9ad4d0-c7fb-41fb-d365-f8c7e027dc43"
      },
      "source": [
        "env.step(1)\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFFFFFF\n",
            "\u001b[41mF\u001b[0mFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tqxPFh9nik7"
      },
      "source": [
        "Now the agent has moved downwards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOVZtkP0n4Rw"
      },
      "source": [
        "Instead of having to render the environment every time to figure out where the agent is, we want to store all the important information in variables. Let's reset the environment and call `step` again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFetwDIdfkx_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "45150160-3fc8-46ee-829b-758c6da662c0"
      },
      "source": [
        "# Reset the environment, and store the initial state in the \"obs\" variable.\n",
        "# When the agent percieves the state, we call this the agent's observation (\"obs\" stands for observation).\n",
        "obs = env.reset()\n",
        "print(\"initial state =\",obs)\n",
        "\n",
        "\n",
        "# take a step to the right (remember, right is denoted by \"2\")\n",
        "obs, reward, done, info = env.step(2) # (focus on \"obs\" for now. \"reward\", \"done\", and \"info\" will be explained in a moment)\n",
        "print(\"new state =\", obs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial state = 0\n",
            "new state = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmM0tzWkoUgk"
      },
      "source": [
        "If we count the squares in the environment starting from the upper left (and start counting from 0), we see that the agent is in square # 1. We can verify this by rendering the environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrT94_jMfnM0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "76b0de63-cdce-4f4d-a685-cb47f227f9b0"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-GrO9ZWon-Z"
      },
      "source": [
        "Rendering the environment also tells us the most recent action we have taken. In this case, we just took action `2`, which is to move right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4kCZpVcpONC"
      },
      "source": [
        "Remember, when we call `step()`, it returns more information than just the new state. It also returns the reward, whether the episode is finished, and a dictionary of any extra information.\n",
        "\n",
        "Let's take another step to the right and look at each of these variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u3i3J97j4Au",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9e89caed-eb53-4abc-8b78-9972ec63c951"
      },
      "source": [
        "obs, reward, done, info = env.step(2)\n",
        "print(\"observation =\", obs)\n",
        "print(\"reward =\", reward)\n",
        "print(\"is the episode done? done =\", done)\n",
        "print(\"(we won't be using info, but here it is: info =\", info, \")\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation = 2\n",
            "reward = 0.0\n",
            "is the episode done? done = False\n",
            "(we won't be using info, but here it is: info = {'prob': 1.0} )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0MuYAr9q70Y"
      },
      "source": [
        "What have we learned from these variables? We see that the agent is in state `2`, it has received no reward, and the episode is not finished.\n",
        "\n",
        "We can interpret zero reward as neutral, neither good or bad.\n",
        "\n",
        "Render the environment to make sure you know where state `2` is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBSUvfCmq2XZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "9f58e513-4655-4235-e05b-b8026b3148c1"
      },
      "source": [
        "env.render()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOcVo5xqr0YB"
      },
      "source": [
        "Now, you know how to move around, you know how to check where the agent currently is (this is the same as checking the current state), and you know how to check if the episode is done.\n",
        "\n",
        "Spend some time controlling the agent in this environment. After each step, print out the current state, the reward the agent has just received, and the `done` variable. You may also call `render()` to render the environment.\n",
        "\n",
        "(You can make lots of code cells to do this.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlEQ_KgFttFW"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "obs, reward,done, info = env.step(2)\n",
        "env.render()\n",
        "print(obs, reward, done, info)\n",
        "\n",
        "if done == True and reward == 1:\n",
        "  print(\"Good job!\")\n",
        "elif done == True and reward == 0:\n",
        "  print(\"Ya dead.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYi9_7zRts4d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imlpRgVXtsVP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXTS8b65scFr"
      },
      "source": [
        "# you can make more code cells below this one by clicking \"[+] Code\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHvt05qmsfFv"
      },
      "source": [
        "What did you notice about this environment? Specifically:\n",
        "\n",
        "1. What are the different rewards the agent received?\n",
        "\n",
        "> At G, the agent gets a reward, otherwise, it gets nothing. \n",
        "\n",
        "2. When did the episode end (when did `done = True`)?\n",
        "\n",
        "> The episode ended when we got into a hole or got to G. \n",
        "\n",
        "3. What reward did the agent receive when the episode ended?\n",
        "\n",
        "> If we reached G, it got a reward of 1.0.\n",
        "\n",
        "Type your answers below each question.\n",
        "\n",
        "\n",
        "(Continue experimenting above until you know the answers to all of these questions.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_mA9gLNldtn"
      },
      "source": [
        "There's a more efficient way of interacting with the environment than typing out `env.step()` every time you want the agent to move.\n",
        "\n",
        "Write a function that takes in a string as an input and renders the current state of the environment. Your function should keep asking for the next action until the episode is over."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2Ip3SR_l4nY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "998d44b7-13e8-4d39-b9cd-3cbdb45b90cc"
      },
      "source": [
        "# Your code here\n",
        "\n",
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "def move():\n",
        "  done = False\n",
        "  while done == False:\n",
        "    move = int(input(\"Move?\"))\n",
        "    obs, reward,done, info = env.step(move)\n",
        "    env.render()\n",
        "  return done, reward\n",
        "    \n",
        "\n",
        "DONE, REWARD = move()\n",
        "\n",
        "\n",
        "while DONE == True:\n",
        "  print(REWARD)\n",
        "  if REWARD == 1:\n",
        "    print(\"Good job!\")\n",
        "    play_again = input(\"Play again?\")\n",
        "    if play_again == \"yes\" or play_again == \"Yes\":\n",
        "      env.reset()\n",
        "      env.render()\n",
        "      DONE, REWARD = move()\n",
        "    else:\n",
        "      print(\"Goodbye!\")\n",
        "      break\n",
        "  elif REWARD == 0:\n",
        "    print(\"Ya dead.\")\n",
        "    revive = input(\"Do you want to come back to life?\")\n",
        "    if revive == \"Yes\" or revive == \"yes\":\n",
        "      env.reset()\n",
        "      env.render()\n",
        "      DONE, REWARD = move()\n",
        "    else:\n",
        "      print(\"Goodbye!\")\n",
        "      break\n",
        "\n",
        "    \n",
        "\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?2\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?2\n",
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?2\n",
            "  (Right)\n",
            "SFF\u001b[41mF\u001b[0mFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?1\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFF\u001b[41mF\u001b[0mFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?1\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFF\u001b[41mH\u001b[0mFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "0.0\n",
            "Ya dead.\n",
            "Do you want to come back to life?yes\n",
            "\n",
            "\u001b[41mS\u001b[0mFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?2\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?2\n",
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?2\n",
            "  (Right)\n",
            "SFF\u001b[41mF\u001b[0mFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?2\n",
            "  (Right)\n",
            "SFFF\u001b[41mF\u001b[0mFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?2\n",
            "  (Right)\n",
            "SFFFF\u001b[41mF\u001b[0mFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?2\n",
            "  (Right)\n",
            "SFFFFF\u001b[41mF\u001b[0mF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?2\n",
            "  (Right)\n",
            "SFFFFFF\u001b[41mF\u001b[0m\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?1\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFF\u001b[41mF\u001b[0m\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?1\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFF\u001b[41mF\u001b[0m\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?1\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHF\u001b[41mF\u001b[0m\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?1\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFF\u001b[41mF\u001b[0m\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?1\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFH\u001b[41mF\u001b[0m\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Move?1\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFH\u001b[41mF\u001b[0m\n",
            "FFFHFFFG\n",
            "Move?1\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n",
            "1.0\n",
            "Good job!\n",
            "Play again?no\n",
            "Goodbye!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfAEGZSuGZfE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}