{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Deep RL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgite03/bu-ai4all-2019/blob/main/rl/Copy_of_Deep_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMU85Kb_2-u9"
      },
      "source": [
        "# Deep RL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsNcGo8z1c4f"
      },
      "source": [
        "So far we've seen how to train an agent by essentially having it memorize the value of each state and action in a table. But what if there is not a finite number of states? I.e. what if the state is continuous? For example, instead of the agent being on one square in a grid, it could be at certain coordinates in the grid. The state could be anywhere inside a continuous 2-dimensional box. We can't create a table with an infinite number of values, so we need a different way to represent the value of each state.\n",
        "\n",
        "We can do this by training a neural network to represent $Q$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBgGryxTO-LL"
      },
      "source": [
        "## New environment: CartPole\n",
        "\n",
        "For this part of the project you'll be working with a different environment, the CartPole environment. Read about what this environment is [here](https://gym.openai.com/envs/CartPole-v1/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgeZ9z8i2yyD"
      },
      "source": [
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyeHweXN9-ul"
      },
      "source": [
        "env = gym.make('CartPole-v1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA2zVdpz-GnM"
      },
      "source": [
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYJz2bpcRyP_"
      },
      "source": [
        "On Google Colab, we can't render the CartPole environment. But that's ok, because when you finish training your model, you can download the model and render the environment states on your local machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCwCrRTJ-OlV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "ff3216ee-4e20-463f-8987-70fda6fd1fb3"
      },
      "source": [
        "env.render() # notice that this won't work"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NoSuchDisplayException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-29498f640354>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# notice that this won't work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     raise ImportError('''\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1894\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1896\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mget_default_display\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDisplay\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \"\"\"\n\u001b[0;32m-> 1845\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wWMOLFYe6B4"
      },
      "source": [
        "You can, however, still print out the states, actions and rewards. Refer to the first Intro to RL notebook to review how to gain information about an OpenAI gym environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB8euXbYfHWV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "ab7df433-802d-46d4-d420-b98a1e675f36"
      },
      "source": [
        "# Your code here. Figure out what the state space and action space are, and play around with the environment.\n",
        "\n",
        "obs = env.reset()\n",
        "print(env.observation_space)\n",
        "\n",
        "print(env.action_space)\n",
        "\n",
        "obs, reward, done, info = env.step(0)\n",
        "print(obs)\n",
        "obs, reward, done, info = env.step(1)\n",
        "print(obs)\n",
        "obs, reward, done, info = env.step(0)\n",
        "print(obs)\n",
        "obs, reward, done, info = env.step(1)\n",
        "print(obs)\n",
        "obs, reward, done, info = env.step(0)\n",
        "print(obs)\n",
        "obs, reward, done, info = env.step(1)\n",
        "print(obs)\n",
        "obs, reward, done, info = env.step(0)\n",
        "print(obs)\n",
        "obs, reward, done, info = env.step(1)\n",
        "print(obs)\n",
        "obs, reward, done, info = env.step(0)\n",
        "print(obs)\n",
        "obs, reward, done, info = env.step(1)\n",
        "print(obs)\n",
        "obs, reward, done, info = env.step(0)\n",
        "print(obs)\n",
        "obs, reward, done, info = env.step(1)\n",
        "print(obs)\n",
        "print(done)\n",
        "print(reward)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box(4,)\n",
            "Discrete(2)\n",
            "[-0.00747024 -0.22507357  0.01645787  0.29915644]\n",
            "[-0.01197171 -0.03019004  0.022441    0.01170912]\n",
            "[-0.01257551 -0.22562651  0.02267518  0.31138712]\n",
            "[-0.01708804 -0.03083483  0.02890293  0.02594064]\n",
            "[-0.01770473 -0.22635909  0.02942174  0.32760082]\n",
            "[-0.02223192 -0.03166809  0.03597376  0.04433946]\n",
            "[-0.02286528 -0.22728693  0.03686055  0.34815187]\n",
            "[-0.02741102 -0.03270812  0.04382358  0.06731645]\n",
            "[-0.02806518 -0.22843008  0.04516991  0.37349753]\n",
            "[-0.03263378 -0.03397791  0.05263986  0.09539223]\n",
            "[-0.03331334 -0.22981327  0.05454771  0.40420734]\n",
            "[-0.0379096  -0.03550563  0.06263185  0.12920847]\n",
            "False\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJdLQuSgPezT"
      },
      "source": [
        "(Just run this next cell to import everything you need.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aheb5pz2-PxV"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import random\n",
        "import tqdm\n",
        "from time import time\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWlCwtl7SM3I"
      },
      "source": [
        "# Building a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEejGNowOems"
      },
      "source": [
        "Build a fully connected neural network using pytorch, that takes the current state as an input and outputs the expected return for moving in each direction.\n",
        "\n",
        "What are the input and output dimensions of the neural network?\n",
        "\n",
        "> input: 1X4\n",
        "\n",
        "> output: 1x2? 2x1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXZiGzWoSeiM"
      },
      "source": [
        "class Q(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(Q, self).__init__()\n",
        "    # weights \n",
        "    self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "    self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.layer3 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.layer4 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.layer5 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.layer6 = nn.Linear(hidden_size, output_size)\n",
        "  def forward(self, inputs):\n",
        "    output = self.layer1(inputs)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    output = self.layer5(output)\n",
        "    output = self.layer6(output)\n",
        "    return output\n",
        "  \n",
        "thingy = Q(4,100,2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMWQpk0SSsgL"
      },
      "source": [
        "# Gain Experience"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkLTQOefQwFG"
      },
      "source": [
        "Write a function that uses the current `q` (but doesn't change it yet) to guide the agent in the environment. This function should record a certain number of transitions (`num_transitions`) and return them in a tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIDSmpyOSJSH"
      },
      "source": [
        "Epsilon is the exploration probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMJ33-wjSpiS"
      },
      "source": [
        "def get_new_experience(thingy, env, num_transitions, epsilon):\n",
        "  # returns experience array: (state(4), action(1), new state(4), reward(1))\n",
        "\n",
        "  # initialize transitions with nan\n",
        "  transitions = torch.full((num_transitions, 10), np.nan)\n",
        "  s = env.reset()\n",
        "  \n",
        "  done = False\n",
        "\n",
        "  for i in tqdm.tqdm(range(num_transitions)):\n",
        "  \n",
        "    if done == True:\n",
        "      s = env.reset()\n",
        "      done = False\n",
        "    \n",
        "    somethin = random.random()\n",
        "    if somethin < epsilon:\n",
        "      a = random.randint(0,1)\n",
        "    else:\n",
        "      return_list = thingy\n",
        "      if return_list[0] > return_list[1]:\n",
        "        a = 0 \n",
        "      else:\n",
        "        a = 1\n",
        "    \n",
        "    old_s=torch.from_numpy(s)\n",
        "    s, reward, done, info = env.step(a)\n",
        "    \n",
        "    transitions[i,:4] = old_s\n",
        "    transitions[i,4:5] = a\n",
        "    transitions[i,5:9] = s\n",
        "    transitions[i,9] = reward\n",
        "    \n",
        "  return transitions\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-d_7S_NSdDH"
      },
      "source": [
        "(Just run this next cell, it's for later.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xpMFuIQScUb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aae1dcb7-ddc5-467e-cc70-4ab1ce5d34a7"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1xXQdWXSwsE"
      },
      "source": [
        "# Define the training function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVaQrJyxSphE"
      },
      "source": [
        "Write some code that trains the model. You can write it in a function so that it is easy to call multiple times for different hyperparameters.\n",
        "\n",
        "The training should go like this:\n",
        "1. Gain experience\n",
        "2. Use experience to refine $Q$\n",
        "3. Repeat\n",
        "\n",
        "When training, the loss should follow the Bellman equation. You should use the loss between $Q(s,a)$ and $r+\\gamma Q(s',a')$ to train $Q$ (which is instantiated earlier in the notebook as lower-case `q`).\n",
        "\n",
        "\n",
        "Now, when training you don't want to completely forget old experience. You will want to keep some of the experience from previous \"gain experience\" steps in your experience buffer.\n",
        "\"`num_transitions`\" is the amount of new experience you will generate in each training iteration, and \"`buffer_size`\" is the total amount of experience you remember at any time. Each time we do step 1, we replace *some* of the old experience in the buffer with new experience you get from calling `get_new_experience()`. The buffer is essentially a giant tensor.\n",
        "\n",
        "Use slicing to extract the correct parts of the transitions from the buffer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23Mq2ppMSwIG"
      },
      "source": [
        "def train(thingy, gamma, criterion, optimizer, num_epochs, num_transitions, buffer_size):\n",
        "    # start an array to keep track of loss\n",
        "    keep_track_of_loss = []\n",
        "    running_loss = 0\n",
        "    \n",
        "    # set up an array to keep track of how well q performs at each epoch\n",
        "    episode_lengths = np.zeros(num_epochs)\n",
        "    \n",
        "    # initialize exploration probability\n",
        "    epsilon = 0.9\n",
        "    \n",
        "    # define the buffer as \"experience\"\n",
        "    experience = get_new_experience(thingy, env, buffer_size, epsilon)#.to(device) # .to(device) moves the experience to the gpu (if you have one)\n",
        "\n",
        "    for epch in tqdm.tqdm(range(num_epochs)):\n",
        "        # replace some of the old experience with new experience\n",
        "        experience = torch.cat((experience[num_transitions:], \n",
        "                                get_new_experience(thingy, env, num_transitions, epsilon)), \n",
        "                               dim=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        state = experience[:,:4]\n",
        "        a = experience[:,4:5]\n",
        "        snew = experience[:,5:9]\n",
        "        reward = experience[:,9]\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "        # ------------------\n",
        "        # | YOUR CODE HERE |\n",
        "        # ------------------\n",
        "        \n",
        "        \n",
        "        # forward function\n",
        "        q_predicted = thingy(state)\n",
        "        the_actions_taken = a.view(-1,1).long() # either 1 or 0\n",
        "        indicies = torch.arange(buffer_size, dtype=torch.float32).unsqueeze(dim=0).long() # the transition number\n",
        "        q_predicted = q_predicted[(indicies, the_actions_taken)].view(-1,1)\n",
        "        \n",
        "        #Q(s',a')\n",
        "        q_next_optimal = torch.max(thingy(snew), dim=1).values.unsqueeze(dim=1)\n",
        "        \n",
        "        # rewards + gamma * q(state)\n",
        "        output_d = reward + gamma * q_next_optimal\n",
        "        \n",
        "        \n",
        "        # zero the gradients in the weights\n",
        "        optimizer.zero_grad()\n",
        "        # calculate the losses\n",
        "        loss = criterion(q_predicted, output_d)\n",
        "        # calculate the gradients\n",
        "        loss.backward()\n",
        "        # update the weights\n",
        "        optimizer.step()\n",
        "        #Log the log so we can plot it later\n",
        "        losses.append(loss.item())\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        # in every epoch, check episode length -- take an average of 3\n",
        "        step_counter= 0\n",
        "        for num in range(3):\n",
        "            s = env.reset()\n",
        "            s = torch.Tensor(s).to(device)\n",
        "            done= False\n",
        "            while not done:\n",
        "                a = torch.argmax(q(s)).cpu()\n",
        "                s_new, r, done, num = env.step(a.numpy())\n",
        "                s = torch.Tensor(s_new).to(device)\n",
        "                step_counter += 1\n",
        "        episode_lengths[epch] = step_counter/3.\n",
        "\n",
        "        # every 10th epoch, record the average loss over the last 10 epochs\n",
        "        running_loss += loss/buffer_size\n",
        "        if epch%10==9:\n",
        "            keep_track_of_loss.append(running_loss/10.)\n",
        "            running_loss=0\n",
        "\n",
        "        # update epsilon\n",
        "        epsilon *= 0.999\n",
        "\n",
        "    # return the episode lengths so we can see how the model progressed, and also the losses so we can plot them\n",
        "    return episode_lengths, keep_track_of_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8okUEYBtS64O"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlkf4CZWZWPd"
      },
      "source": [
        "Create our environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmxG58cZTH2Q"
      },
      "source": [
        "env = gym.make('CartPole-v1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujjKn__6behV"
      },
      "source": [
        "Here's some starter code. However, you should run lots of tests, and it will probably be helpful for you to run this in a loop and record all the different results for examination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfxjHDN_S04L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "2ae28922-f965-4d2d-f844-a0b7fb2b054c"
      },
      "source": [
        "# instantiate the model q\n",
        "thingy2 = Q(4, 100, 2)#.to(device) # .to(device) sends the model to the GPU. The GPU makes training faster\n",
        "\n",
        "# define our criterion and optimizer. You can use the Pytorch notebook as reference, \n",
        "# and also look at the pytorch documentation if you want to try some different loss functions\n",
        "# and optimizers\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(thingy2.parameters(), lr=0.5)\n",
        "\n",
        "# define hyperparameters. Try a few different combinations and record which ones do the best.\n",
        "gamma = 0.9\n",
        "num_epochs = 10000 \n",
        "num_transitions = 10000\n",
        "buffer_size = 20000\n",
        "\n",
        "# call the training function\n",
        "episode_lengths, all_losses = train(thingy, gamma, criterion, optimizer, num_epochs, num_transitions, buffer_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/20000 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6c4c67c7556a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# call the training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mepisode_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthingy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_transitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-c17c21aa59cb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(thingy, gamma, criterion, optimizer, num_epochs, num_transitions, buffer_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# define the buffer as \"experience\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mexperience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_new_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthingy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to(device) # .to(device) moves the experience to the gpu (if you have one)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-dde5f139434e>\u001b[0m in \u001b[0;36mget_new_experience\u001b[0;34m(thingy, env, num_transitions, epsilon)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't assign a numpy.ndarray to a torch.FloatTensor"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R--ISgMJaWxe"
      },
      "source": [
        "# Examine the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMpMbov1Z6d4"
      },
      "source": [
        "Define some useful functions to look at our episode lengths and loss curve:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r63vUBlcZ-_Y"
      },
      "source": [
        "def plot_episode_lengths(episode_lengths):\n",
        "  plt.scatter(range(len(episode_lengths)), episode_lengths, marker=\".\")\n",
        "  plt.show()\n",
        "  \n",
        "def plot_loss_curve(all_losses):\n",
        "  plt.plot([10*e for e in range(len(all_losses))], all_losses)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i8m1Rnncscu"
      },
      "source": [
        "Plot the performace of the models that you've trained. See if you have trained a good one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60fT1melXQyO"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}