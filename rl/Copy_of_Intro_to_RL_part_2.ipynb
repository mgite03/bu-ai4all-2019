{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Intro to RL part 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgite03/bu-ai4all-2019/blob/main/rl/Copy_of_Intro_to_RL_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J_7LY1hVX4N"
      },
      "source": [
        "## Review From Part 1\n",
        "\n",
        "Define the following terms:\n",
        "\n",
        "1. Agent\n",
        "\n",
        "> The object in the environment that completes actions.\n",
        "\n",
        "2. Environment\n",
        "\n",
        "> The space where the agent performs actions.\n",
        "\n",
        "3. State (and MDP)\n",
        "\n",
        "> The location of the agent, and description of environment. States are independent from previous ones. \n",
        "\n",
        "4. Action\n",
        "\n",
        "> What the agent does to change its environment/state. The agent recieves a reward for certain actions. \n",
        "\n",
        "5. Episode\n",
        "\n",
        "> The collection of actions that the agent does, ends in reward or death. \n",
        "\n",
        "6. Timestep\n",
        "\n",
        "> The entire process of doing an action and receiving/not receiving an award. (Episodes are made of timesteps.)\n",
        "\n",
        "Make sure you fully understand the definitions (and implications) of each of these terms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfieXXi0V033"
      },
      "source": [
        "# Intro to RL part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnPh-v0LHbGw"
      },
      "source": [
        "## Return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDtd0sVKmC6v"
      },
      "source": [
        "Now that we know how to interact with an environment, how do we make our agent intelligent?\n",
        "\n",
        "Our goal is to teach the agent how to choose the best action for any state it happens to be in. \n",
        "\n",
        "But how do we define \"best\"? Should the agent choose the actions that give it a reward most quickly (favoring gaining rewards in closer timesteps), or should it hold out for a bigger reward further in the future? We weigh these options against each other by defining \"return\" $G$:\n",
        "\n",
        "$$G = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^T \\gamma^{k} R_{t+1 + k}$$\n",
        "\n",
        "$$G_{t} = R_{t+1} + \\gamma G_{t+1}$$\n",
        "\n",
        "where $R_{t+1+k}$ is the reward at each timestep, and $\\gamma$ is a discount factor (a number between 0 and 1).\n",
        "\n",
        "This is a mathematical expression of how valuable a certain sequence of states and actions is -- it weights sooner return more heavily than later return. $\\gamma$ is a hyperparameter, meaning that we can change it to make the agent care more or less about rewards further in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7NPTv3j5AT1"
      },
      "source": [
        "To make sure you understand return, write a function that takes in $\\gamma$ and a sequence of future rewards starting at the current timestep, and return (from the function) the return $G$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6zVWPZ15Nal"
      },
      "source": [
        "def get_return(gamma, rewards):\n",
        "  k = 0\n",
        "  ret = 0\n",
        "  for reward in rewards:\n",
        "    ret += (gamma**k)*reward\n",
        "    k+=1\n",
        "  return(ret)\n",
        "    \n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9jQ8vI95Ucc"
      },
      "source": [
        "Test your function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmIMf2Pj5WEn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b4afc271-08d8-4800-a075-7e881c238935"
      },
      "source": [
        "print(get_return(0.9, [2,3,2,3,2,3,2]))\n",
        "print(get_return(0.9, [0,0,0,1,0,0,0]))\n",
        "print(get_return(0.9, [0,0,0,0,0,1,0]))\n",
        "print(get_return(0.8, [0,0,0,0,0,1,0]))\n",
        "print(get_return(0.9, [1,0,0,0,0,0,0]))\n",
        "# create more tests to help answer the questions below"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12.653552000000003\n",
            "0.7290000000000001\n",
            "0.5904900000000001\n",
            "0.3276800000000001\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPwfiGu85j5C"
      },
      "source": [
        "How does changing $\\gamma$ affect the return?\n",
        "\n",
        "> making gamma smaller makes the return smaller.\n",
        "\n",
        "How does changing the order of the rewards affect the return?\n",
        "\n",
        "> Rewards given sooner return greater values. Greater rewards also give a greater return."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4anknqsc5xb"
      },
      "source": [
        "## Probability\n",
        "\n",
        "We define $p(s', r \\mid s,a)$ to be the probability of moving to state $s'$ and receiving a reward $r$ given that we are in state $s$ and take action $a$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZox5knKYupA"
      },
      "source": [
        "## Another note about environment\n",
        "\n",
        "In the previous notebook we worked with the FrozenLake environment. In this environment, whenever we took the action \"move down\" (e.g.) the state changed in a deterministic way, meaning we were certain of the next state the agent would enter.\n",
        "\n",
        "Not all environments are deterministic, however. If we set `is_slippery=True` in the FrozenLake environment, when the agent takes the action \"move down\" it is no longer certain that the state will change to the square below. There is a chance that the agent will end up in one of a few different states. This is a *stochastic* environment. We represent the probability of changing states as either $p(s' \\mid s, a)$ (which is equal to $\\sum_{r\\in\\mathcal{R}} p(s', r \\mid s, a)$), or as $\\mathcal{P}_{s,s'}^a$. ($r \\in \\mathcal{R}$ denotes all the possible rewards.) Both of these can be read as \"the probability of moving to state $s'$ given that we are currently in state $s$ and we take action $a$\".\n",
        "\n",
        "### Even further complexity\n",
        "\n",
        "Often, in real world examples, the agent is not able to see the complete state of the environment. While the state of the environment may currently be $s_e$, the agent may observe the state to be $s_a \\neq s_e$. This is called a partially-observable environment.\n",
        "\n",
        "Rewards are rarely certain, either. Sometimes we are not sure what reward we will get for moving into a certain state. We might get a different reward from an environment even if we take the same actions and end up in the same states.\n",
        "\n",
        "In this project, though, we will only work with fully-observable environments and deterministic rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hY4JQWedtb5"
      },
      "source": [
        "## Expected Reward / Expected Return\n",
        "\n",
        "Even though we don't always know exactly what reward we will receive, we can still make decisions based on what we *expect* the reward to be. It is essentially a weighted average of all the possible rewards based on how probable they are.\n",
        "\n",
        "The expectation of a reward $R_t$ at timestep $t$ given some state $s_{t-1}$ at timestep $t-1$ and action $a_{t-1}$ at $t-1$ is written as $$\\mathbb{E}[R_t \\mid S_{t-1}=s, A_{t-1}=a]$$. This can be expressed as $$\\sum_{r\\in\\mathcal{R}} r \\sum_{s'\\in\\mathcal{S}} p(s', r \\mid s, a)$$ using the probability notation from before.\n",
        "\n",
        "**Question**: How would you express expected reward given $s$, $a$, and $s'$?\n",
        "\n",
        "*Hint: what are you given, and what do you not know?*\n",
        "\n",
        "> $\\sum_{r\\in\\mathcal{R}} r *p( s', r\\mid  s, a ) /  p(s' \\mid s, a)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZf-TLKNHlpm"
      },
      "source": [
        "## Policy\n",
        "\n",
        "In order to choose an action, an agent will follow a policy function, $\\pi$. We define $\\pi(s,a)$ to be the probability that the agent will take action $a$ when it is in state $s$. If the agent follows a deterministic policy, $\\pi(s,a) = 1$ for any given $s$ and $a$. If the agent follows a stochastic policy, then $\\pi(s,a) < 1$ for any given $s$ and $a$. This means that the agent won't always take the same action in the same state.\n",
        "\n",
        "The agent must learn a good policy in order to take actions that will lead to a high return."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKj-RTPoYGS8"
      },
      "source": [
        "## State-value and Action-value functions\n",
        "\n",
        "There are two main ways to think about how to achieve a high return: using the state-value function or using the action-value function. Each of these functions express the expected return if the agent follows a specific policy $\\pi$ and given $s$ or $s$ and $a$.\n",
        "\n",
        "1. **State-value** function\n",
        "\n",
        "The state-value function operates by associating each *state* with a reward. We define a function $v : S \\rightarrow R$ as $$v(s) = \\mathbb{E}[G_t \\mid S_{t-1}=s]$$\n",
        "\n",
        "\n",
        "2. **Action-value** function\n",
        "\n",
        "The action-value function is similar to the state-value function, except the action-value function takes into account the agent taking a specific action as well as a state. We define a function $q : S, A \\rightarrow R$ as $$q(s,a) = \\mathbb{E}[G_t \\mid S_{t-1}=s, A_{t-1}=a]$$\n",
        "\n",
        "**Question**: How would you express the state-value function in terms of the action-value function?\n",
        "\n",
        "> $v(s) = $\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UVralwj8VxV"
      },
      "source": [
        "<hr>\n",
        "\n",
        "Note: If you just get through this first part today, that's fine\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2MVYrLoqPo-"
      },
      "source": [
        "## Q Learning\n",
        "\n",
        "Q learning is one approach to RL that focuses on the action-value function, $q$. The policy $\\pi$ is implicitly defined by taking the actions that lead to the highest return, according to $q$. You will sometimes see $q$ written as $q_\\pi$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjMqQjuLmTVr"
      },
      "source": [
        "However, there's an issue. We don't know what the return is for any of the states and actions. We need to figure out the return of each state and action by allowing the agent to move around its environment and keeping track of the rewards it receives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsTc50cavUQo"
      },
      "source": [
        "Create a table to store what we think is the return for each state and action in the FrozenLake environment.\n",
        "\n",
        "(You want an entry for each state and each action. Set the dimensions of the table so that you can address it as `Q[s][a]`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33v-DAqzvcME",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "c7c81422-1841-4e46-d474-a90caf5ef7c8"
      },
      "source": [
        "Q = [[0 for a in range(4)] for s in range(16)] # edit this line\n",
        "# this is a fancy way of saying \"create a ? x ? matrix and initialize all the values to 0\"\n",
        "\n",
        "\n",
        "\n",
        "def printQ(Q):\n",
        "  for i in range(len(Q)):\n",
        "    print(Q[i])\n",
        "\n",
        "\n",
        "printQ(Q)\n",
        "\n",
        "#Q[2][3] = 2  Q[state][action]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6AnqKBWvct4"
      },
      "source": [
        "We will define our policy $\\pi$ to choose the action that will lead to the highest-valued next expected state. Write a function that would choose the optimal action given the current `Q` and current state (since the values of `Q` are not yet filled in, you don't have to test this function yet):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2AEg1f0u2iC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ef4c292-2ca1-4c38-e131-3da6765a6512"
      },
      "source": [
        "import random\n",
        "def choose_optimal_a(s,Q):\n",
        "    check_up = Q[s][3]\n",
        "    check_down = Q[s][1]\n",
        "    check_left = Q[s][0]\n",
        "    check_right = Q[s][2]\n",
        "    best_a = max(check_up, check_down, check_right, check_left)\n",
        "    if check_up == check_down and check_left == check_right and check_up == check_right:\n",
        "      a_index = random.randint(0,3)\n",
        "    elif check_up==best_a:\n",
        "      a_index = 3\n",
        "    elif check_down==best_a:\n",
        "      a_index = 1\n",
        "    elif check_left==best_a:\n",
        "      a_index = 0\n",
        "    elif check_right==best_a:\n",
        "      a_index = 2\n",
        "    return a_index\n",
        "choose_optimal_a(1,Q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-rSfNN5vIEQ"
      },
      "source": [
        "Every time the agent takes a certain action and the environment returns a certain new state and reward, we want to move the values in `Q` closer to their true values. Remember, in our case, $\\pi$ is to always take the action that we think has the highest return, according to `Q`. In this way, when we update `Q` we are also updating $\\pi$\n",
        "\n",
        "To update the table, we follow the Bellman equation:\n",
        "\n",
        "$$q_\\pi(s,a) = r + \\gamma \\max_{a'}q(s',a') $$\n",
        "\n",
        "Intuitively, this function is looking back at the expected return for $s$ and $a$ given that we have now taken that we have taken a step forward. It is like using hindsight to make better predictions in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIc9j8p44iDu"
      },
      "source": [
        "However, since we are continuously updating $q$ (and therefore $\\pi$), we don't want to completely override the previous values of `Q`, as that would not make for a stable learning process. We want to ajust them slightly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9xkVkC-vsNz"
      },
      "source": [
        "Write some code that updates your table according to the Bellman equation (it should take an action, receive a reward, and update the appropriate entry in `Q`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYGdI7juvuZE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a042b02-8784-459a-e76e-153532f7646a"
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"FrozenLake-v0\", is_slippery=True)\n",
        "s=2\n",
        "a=0\n",
        "Q[s][a]=0\n",
        "r=2\n",
        "check=[]\n",
        "for i in range(4):\n",
        "  check+=[Q[s][i]]\n",
        "gamma=.1\n",
        "Q[s][a] = r + gamma*(max(check))\n",
        "print(Q[s][a])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_HoeZrtWdvT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "490947de-c789-4d7a-badf-9dcae8585a7e"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "for i in tqdm(range(10)):\n",
        "  time.sleep(.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9oHGZXkvwqR"
      },
      "source": [
        "We end up with the following training loop. Complete the code inside the loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u82GDTyIrH1_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "f12311c4-d417-4ffa-b073-be710fe53e3e"
      },
      "source": [
        "env = gym.make(\"FrozenLake-v0\", is_slippery=True)\n",
        "\n",
        "\n",
        "# don't forget to reset Q\n",
        "Q = [[0 for a in range(4)] for s in range(16)] \n",
        "\n",
        "epsilon = 0.9\n",
        "\n",
        "\n",
        "num_epochs = 100000\n",
        "\n",
        "for i in tqdm(range(num_epochs)):\n",
        "  s = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    somethin = random.random()\n",
        "    if somethin < epsilon:\n",
        "      a = random.randint(0,3)\n",
        "    else:\n",
        "      a= choose_optimal_a(s,Q)\n",
        "    # take action\n",
        "    old_state = s\n",
        "    s, reward, done, info = env.step(a)\n",
        "    \n",
        "    if s != 15 and done == True:\n",
        "      reward -=1\n",
        "\n",
        "    \n",
        "    # update table\n",
        "    \n",
        "    check=[]\n",
        "    for i in range(4):\n",
        "      check+=[Q[s][i]]\n",
        "    gamma=.9\n",
        "    Q[old_state][a] = (0.9*Q[old_state][a]+ 0.1*(reward + gamma*(max(check))))\n",
        "    epsilon *= 0.99896\n",
        "\n",
        "\n",
        "    \n",
        "env.render()   \n",
        "printQ(Q)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [01:07<00:00, 1488.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "[0.012128197827990248, -0.15904662757905633, -0.1565571818883713, -0.15969154983272316]\n",
            "[-0.27186812117624354, -0.2592871484413749, -0.3581906115736314, 0.018600776299999115]\n",
            "[-0.1989618631290543, -0.2605196705931636, -0.20620202816598543, -0.017675346470209925]\n",
            "[-0.3178637620565129, -0.39762966893698715, -0.324627482720111, -0.03729807723720214]\n",
            "[0.01994916367230312, -0.28653949828186936, -0.27462248457147265, -0.32253965553531855]\n",
            "[0, 0, 0, 0]\n",
            "[-0.7694443886979748, -0.7736455264247891, -0.21205622133195565, -0.7854412530278388]\n",
            "[0, 0, 0, 0]\n",
            "[-0.3234120918003049, -0.44404843231995517, -0.2617772515004455, 0.04891814049225932]\n",
            "[-0.2585903648352459, 0.17354469986448123, -0.1550778336881107, -0.26875492967881753]\n",
            "[0.2930486344038311, -0.2888312508781502, -0.32489638939449017, -0.30834247258373165]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[-0.20517039941985232, -0.1304984312045525, 0.2719072195175513, -0.07859277986364914]\n",
            "[0.11276832939675449, 0.7427208533012898, 0.09760507756772567, 0.0953543969757242]\n",
            "[0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkX0vaWH8CyJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a0a890c4-2451-4fb2-f8d2-0e20af20779f"
      },
      "source": [
        "env = gym.make(\"FrozenLake-v0\", is_slippery=True)\n",
        "s = env.reset()\n",
        "success = 0\n",
        "\n",
        "\n",
        "for k in range(20000):\n",
        "  done = False\n",
        "  s= env.reset()\n",
        "  while done != True:\n",
        "    new_a = choose_optimal_a(s, Q)\n",
        "    s, r, done, info = env.step(new_a)\n",
        "  if s ==15:\n",
        "    success +=1\n",
        "print(success)\n",
        "print(success/20000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14816\n",
            "0.7408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQwxHh29rf7C"
      },
      "source": [
        "Now watch your agent move quickly across the frozen lake without falling into any holes! Write a function that uses the Q table you trained to guide and render the agent moving through the environment. You can adapt your code from the last question of the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7Bs8xe8rxzA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "beae3fe6-58e4-4bcf-b31a-1c9e6bd45e42"
      },
      "source": [
        "import random\n",
        "\n",
        "env = gym.make(\"FrozenLake-v0\", is_slippery=True)\n",
        "\n",
        "decimal_num=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "\n",
        "# don't forget to reset Q\n",
        "Q = [[0 for a in range(4)] for s in range(16)] \n",
        "\n",
        "for i in range(len(decimal_num)):\n",
        "  success2=0\n",
        "  epsilon=decimal_num[i]\n",
        "  for j in range(len(decimal_num)):\n",
        "    gamma=decimal_num[j]\n",
        "    for k in range(len(decimal_num)):\n",
        "      learning_r=decimal_num[k]\n",
        "      l_rate2=1-learning_r\n",
        "\n",
        "\n",
        "  num_epochs = 100000\n",
        "\n",
        "  for i in tqdm(range(num_epochs)):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      somethin = random.random()\n",
        "      if somethin < epsilon:\n",
        "        a = random.randint(0,3)\n",
        "      else:\n",
        "        a= choose_optimal_a(s,Q)\n",
        "      # take action\n",
        "      old_state = s\n",
        "      s, reward, done, info = env.step(a)\n",
        "\n",
        "      if s != 15 and done == True:\n",
        "        reward -=1\n",
        "\n",
        "\n",
        "      # update table\n",
        "\n",
        "      check=[]\n",
        "      for i in range(4):\n",
        "        check+=[Q[s][i]]\n",
        "      Q[old_state][a] = (l_rate2*Q[old_state][a]+ learning_r*(reward + gamma*(max(check))))\n",
        "      epsilon *= 0.999999\n",
        "\n",
        "  env = gym.make(\"FrozenLake-v0\", is_slippery=True)\n",
        "  s = env.reset()\n",
        "  success = 0\n",
        "\n",
        "\n",
        "  for k in range(20000):\n",
        "    done = False\n",
        "    s= env.reset()\n",
        "    while done != True:\n",
        "      new_a = choose_optimal_a(s, Q)\n",
        "      s, r, done, info = env.step(new_a)\n",
        "    if s ==15:\n",
        "      success +=1\n",
        "    success3=success/20000\n",
        "    if success3> success2:\n",
        "      success2=success3\n",
        "      learning=learning_r\n",
        "      gamma2=gamma\n",
        "      epsilon2=epsilon\n",
        "print(\"learning rate:\",learning)\n",
        "print(\"gamma:\",gamma2)\n",
        "print(\"epsilon:\",epsilon2)\n",
        "      \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:18<00:00, 5387.68it/s]\n",
            "100%|██████████| 100000/100000 [00:43<00:00, 2323.81it/s]\n",
            "100%|██████████| 100000/100000 [00:39<00:00, 2515.98it/s]\n",
            "100%|██████████| 100000/100000 [00:34<00:00, 2891.57it/s]\n",
            "100%|██████████| 100000/100000 [00:26<00:00, 3824.34it/s]\n",
            "100%|██████████| 100000/100000 [00:22<00:00, 4354.20it/s]\n",
            "100%|██████████| 100000/100000 [00:19<00:00, 5234.27it/s]\n",
            "100%|██████████| 100000/100000 [00:18<00:00, 5424.68it/s]\n",
            "100%|██████████| 100000/100000 [00:15<00:00, 6411.19it/s]\n",
            "100%|██████████| 100000/100000 [00:16<00:00, 6186.89it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "learning rate: 0.9\n",
            "gamma: 0.9\n",
            "epsilon: 0.3519951136791793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrCbHC08skBS"
      },
      "source": [
        "#### Additional Resources\n",
        "\n",
        "* [https://towardsdatascience.com/introduction-to-q-learning-88d1c4f2b49c](https://towardsdatascience.com/introduction-to-q-learning-88d1c4f2b49c)"
      ]
    }
  ]
}